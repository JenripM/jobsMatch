from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from schemas import Match, PromptRequest
from models import obtener_practicas, obtener_practicas_recientes
from buscar_practicas_afines import buscar_practicas_afines
from pydantic import BaseModel
from google.cloud.firestore_v1.vector import Vector
import json
import time
import asyncio
from typing import AsyncGenerator
import logging
import os

# Configuraci√≥n para streaming puro (sin compresi√≥n)
STREAMING_CHUNK_SIZE = 1   # 1 pr√°ctica por chunk para m√°xima velocidad
STREAMING_ENABLED = True   # Flag para habilitar/deshabilitar streaming
USE_PURE_STREAMING = True  # Streaming sin compresi√≥n para latencia m√≠nima

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Request schema for the new endpoint
class CVEmbeddingRequest(BaseModel):
    cv_url: str
    desired_position: str | None

app = FastAPI()

# Configuraci√≥n de CORS (SIN GZipMiddleware para streaming puro)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

import gzip
import json
from datetime import datetime
from fastapi import Request, Response
from google.api_core.datetime_helpers import DatetimeWithNanoseconds





def custom_json_serializer(obj):
    """Serializer personalizado para manejar tipos especiales de Firestore"""
    if isinstance(obj, DatetimeWithNanoseconds):
        return obj.isoformat()
    elif isinstance(obj, datetime):
        return obj.isoformat()
    raise TypeError(f"Object of type {type(obj)} is not JSON serializable")

async def generate_ndjson_streaming_practices(practicas: list, timing_stats: dict) -> AsyncGenerator[str, None]:
    """
    Generador as√≠ncrono que produce pr√°cticas en formato NDJSON streaming puro.
    
    NDJSON STREAMING: Cada pr√°ctica se env√≠a como una l√≠nea JSON separada.
    El frontend puede procesar cada l√≠nea inmediatamente sin esperar el JSON completo.
    
    Args:
        practicas: Lista de pr√°cticas a enviar
        timing_stats: Estad√≠sticas de tiempo del procesamiento
    
    Yields:
        str: L√≠neas NDJSON (una pr√°ctica por l√≠nea + metadata al final)
    """
    logger.info(f"üöÄ Iniciando NDJSON streaming de {len(practicas)} pr√°cticas")
    logger.info(f"üìù Formato: Una l√≠nea JSON por pr√°ctica")
    
    try:
        # Procesar pr√°cticas individualmente como l√≠neas NDJSON
        total_practicas = len(practicas)
        
        for practica_index, practica in enumerate(practicas):
            logger.info(f"üì¶ Enviando pr√°ctica {practica_index + 1}/{total_practicas}")
            
            # Serializar pr√°ctica individual como l√≠nea JSON
            practica_json = json.dumps(practica, ensure_ascii=False, default=custom_json_serializer)
            
            # Enviar pr√°ctica como l√≠nea NDJSON (con salto de l√≠nea)
            yield f"{practica_json}\n"
            
            # Pausa m√≠nima para permitir procesamiento progresivo
            await asyncio.sleep(0.05)  # 50ms por pr√°ctica
        
        # Preparar metadata como √∫ltima l√≠nea NDJSON
        metadata = {
            "metadata": {
                "total_practicas_procesadas": len(practicas),
                "streaming": True,
                "chunk_size": STREAMING_CHUNK_SIZE,
                "timing_stats": timing_stats
            }
        }
        
        # Enviar metadata como √∫ltima l√≠nea NDJSON
        metadata_json = json.dumps(metadata, ensure_ascii=False, default=custom_json_serializer)
        yield f"{metadata_json}\n"
        
        logger.info(f"‚úÖ NDJSON streaming completado exitosamente - {len(practicas)} pr√°cticas + metadata enviadas")
        
    except Exception as e:
        logger.error(f"‚ùå Error durante NDJSON streaming: {e}")
        # Enviar error como l√≠nea NDJSON
        error_data = {
            "error": {
                "message": f"Error durante streaming: {str(e)}",
                "streaming": True,
                "format": "ndjson"
            }
        }
        error_json = json.dumps(error_data, ensure_ascii=False, default=custom_json_serializer)
        yield f"{error_json}\n"

@app.post("/match-practices")
async def match_practices(request: Request):
    """
    Endpoint optimizado para matching de pr√°cticas
    Mejoras implementadas:
    - Soporte para cv_url O cv_embeddings como par√°metros
    - Si se pasa cv_embeddings, se ejecuta directamente la b√∫squeda vectorial ( mas rapido )
    - Si se pasa cv_url, se genera el embedding y luego se procede a hacer la b√∫squeda vectorial ( mas lento )
    - Soporte para compresi√≥n gzip bidireccional (request y response)
    - Medici√≥n detallada de tiempos por etapa
    """
    # Iniciar medici√≥n de tiempo total
    start_total = time.time()
    timing_stats = {}
    
    try:
        # 1. ETAPA: Descompresi√≥n de request
        start_decompress = time.time()
        content_encoding = request.headers.get("content-encoding", "").lower()
        
        if content_encoding == "gzip":
            # Leer datos comprimidos
            compressed_data = await request.body()
            print(f"üóúÔ∏è Datos comprimidos recibidos: {len(compressed_data)} bytes")
            
            # Descomprimir usando gzip
            decompressed_data = gzip.decompress(compressed_data)
            raw_data = json.loads(decompressed_data.decode('utf-8'))
            print(f"üì¶ Datos descomprimidos: {len(decompressed_data)} bytes")
            
        else:
            # Datos sin comprimir (fallback)
            raw_data = await request.json()
            print(f"üìÑ Datos sin comprimir recibidos")
        
        timing_stats['request_decompression'] = time.time() - start_decompress
        
        # 2. ETAPA: Procesamiento de datos
        start_processing = time.time()
        
        print(f"üîç DATOS RAW DEL FRONTEND:")
        print(f"   - Campos enviados: {list(raw_data.keys())}")
        
        # Crear objeto Match manualmente desde los datos descomprimidos
        match = Match(**raw_data)
        
        timing_stats['data_processing'] = time.time() - start_processing
        
        print(f"\nüîÑ Iniciando matching para puesto: {match.puesto}")
        print(f"üìã Par√°metros PROCESADOS por Pydantic:")
        print(f"   - cv_url: {match.cv_url}")
        print(f"   - puesto: {match.puesto}")
        print(f"   - cv_embeddings: {'‚úÖ Presente' if match.cv_embeddings else '‚ùå Ausente'}")

        if match.cv_embeddings:
            print(f"   - cv_embeddings tipo: diccionario multi-aspecto")
            print(f"   - aspectos disponibles: {list(match.cv_embeddings.keys())}")
            for aspect, embedding in match.cv_embeddings.items():
                print(f"   - {aspect}: {len(embedding)} dimensiones")
        
        # 3. ETAPA: B√∫squeda/Matching
        start_search = time.time()
        
        # Llamar a la funci√≥n con los par√°metros apropiados
        if match.cv_embeddings:
            print(f"üìä Usando embeddings multi-aspecto proporcionados directamente")
            practicas_con_similitud = await buscar_practicas_afines(
                cv_url=None,
                cv_embeddings=match.cv_embeddings, 
                puesto=match.puesto,
                #devolver practicas solo mayores a 0%
                percentage_threshold= 0,
                #solo buscar pr√°cticas recientes (ultimos 5 dias)
                sinceDays=5,
            )
        else:
            print(f"üîó Usando URL del CV: {match.cv_url}")
            practicas_con_similitud = await buscar_practicas_afines(
                cv_url=match.cv_url, 
                cv_embeddings=None,
                puesto=match.puesto,
                #devolver practicas solo mayores a 0%
                percentage_threshold= 0,
                #solo buscar pr√°cticas recientes (ultimos 5 dias)
                sinceDays=5,
            )
        
        timing_stats['search_matching'] = time.time() - start_search
        
        # 4. ETAPA: Preparaci√≥n de respuesta
        start_response_prep = time.time()
        
        # Calcular tiempo total hasta ahora
        timing_stats['total_processing'] = time.time() - start_total
        timing_stats['response_preparation'] = time.time() - start_response_prep
        timing_stats['total_time'] = time.time() - start_total
        
        print(f"\n‚è±Ô∏è ESTAD√çSTICAS DE TIEMPO:")
        print(f"   - Descompresi√≥n request: {timing_stats['request_decompression']:.4f}s")
        print(f"   - Procesamiento datos: {timing_stats['data_processing']:.4f}s")
        print(f"   - B√∫squeda/Matching: {timing_stats['search_matching']:.4f}s")
        print(f"   - Preparaci√≥n respuesta: {timing_stats['response_preparation']:.4f}s")
        print(f"   - üéÜ TIEMPO TOTAL: {timing_stats['total_time']:.4f}s")
        
        # Usar siempre streaming puro sin compresi√≥n
        if STREAMING_ENABLED and len(practicas_con_similitud) > 0:
            print(f"üì° Usando STREAMING PURO - {len(practicas_con_similitud)} pr√°cticas")
            
            # Retornar StreamingResponse sin compresi√≥n
            return StreamingResponse(
                generate_ndjson_streaming_practices(practicas_con_similitud, timing_stats),
                media_type="application/x-ndjson",
                headers={
                    "Content-Type": "application/x-ndjson",
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive"
                }
            )
        else:
            print(f"üìÑ Usando respuesta JSON tradicional - {len(practicas_con_similitud)} pr√°cticas")
            
            # Respuesta tradicional sin compresi√≥n
            response_data = {
                "practicas": practicas_con_similitud[:match.limit],
                "metadata": {
                    "total_practicas_procesadas": len(practicas_con_similitud),
                    "total_practicas_devueltas": match.limit,
                    "streaming": False,
                    "timing_stats": timing_stats
                }
            }
            
            return response_data
            
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Error al parsear JSON")
    except Exception as e:
        print(f"‚ùå Error en match_practices: {e}")
        raise HTTPException(status_code=500, detail=f"Error interno: {str(e)}")


@app.get("/practicas")
def get_all_practicas():
    return obtener_practicas()

@app.get("/practicas-recientes")
def get_recent_practicas():
    return obtener_practicas_recientes()



@app.post("/chatgpt")
async def chatgpt_response(request: PromptRequest):
    respuesta = "SI ESTAS VIENDO ESTO, POR FAVOR NOTIFICANOS. ¬øEN QUE REPOSITORIO ESTAS EJECUTANDO EL C√ìDIGO QUE NECESITA ESTE ENDPOINT? \n\natt: Hector 07/08/2025"
    return {"respuesta": respuesta}



@app.post("/cvFileUrl_to_embedding")
async def cv_file_url_to_embedding(request: CVEmbeddingRequest):
    """
    Endpoint que genera embeddings m√∫ltiples de un CV.
    
    Args:
        request: CVEmbeddingRequest con cv_url y desired_position opcional
    
    Returns:
        dict: JSON con embeddings por aspecto o error
        {
            'embeddings': {
                'hard_skills': vector<2048>,
                'category': vector<2048>,
                'related_degrees': vector<2048>,
                'soft_skills': vector<2048>
            }
        }
    """
    print("üöÄ POST /cvFileUrl_to_embedding")
    
    # Import lazy de cv_to_embeddings solo cuando se necesite
    from models import cv_to_embeddings
    embeddings_dict = await cv_to_embeddings(request.cv_url, request.desired_position)
    
    if embeddings_dict is None:
        return {"error": "No se pudo generar los embeddings del CV"}
    
    # Convertir cada embedding a Vector para la respuesta
    embeddings_response = {}
    for aspect_name, embedding in embeddings_dict.items():
        embeddings_response[aspect_name] = Vector(embedding)
    
    return {
        "embeddings": embeddings_response,
        "aspects_count": len(embeddings_response),
        "available_aspects": list(embeddings_response.keys())
    }
